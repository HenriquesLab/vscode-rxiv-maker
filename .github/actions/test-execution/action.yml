name: 'Test Execution'
description: 'Reusable test runner with coverage and reporting'
inputs:
  test-type:
    description: 'Type of tests to run: unit, integration, performance, security'
    required: false
    default: 'unit'
  python-version:
    description: 'Python version for reporting'
    required: false
    default: '3.11'
  os:
    description: 'Operating system for reporting'
    required: false
    default: 'ubuntu-latest'
  coverage:
    description: 'Enable coverage reporting'
    required: false
    default: 'true'
  max-failures:
    description: 'Maximum number of test failures before stopping'
    required: false
    default: '20'
  timeout:
    description: 'Test timeout in seconds'
    required: false
    default: '300'
  upload-coverage:
    description: 'Upload coverage to Codecov'
    required: false
    default: 'false'

outputs:
  test-result:
    description: 'Test execution result (success/failure)'
    value: ${{ steps.run-tests.outputs.result }}
  coverage-file:
    description: 'Coverage file path'
    value: ${{ steps.run-tests.outputs.coverage-file }}

runs:
  using: 'composite'
  steps:
    - name: Windows-specific cleanup
      if: runner.os == 'Windows'
      shell: powershell
      run: |
        # Force cleanup of any stale processes and files
        Get-Process | Where-Object {$_.ProcessName -match "python|pytest|uv"} | Stop-Process -Force -ErrorAction SilentlyContinue

        # Clean temp directory
        $tempPath = [System.IO.Path]::GetTempPath()
        Get-ChildItem -Path $tempPath -Filter "*rxiv*" -Recurse -Force -ErrorAction SilentlyContinue | Remove-Item -Recurse -Force -ErrorAction SilentlyContinue
        Get-ChildItem -Path $tempPath -Filter "*pytest*" -Recurse -Force -ErrorAction SilentlyContinue | Remove-Item -Recurse -Force -ErrorAction SilentlyContinue

        # Set Windows file handling options
        [System.Environment]::SetEnvironmentVariable("PYTHONDONTWRITEBYTECODE", "1", "Process")
        [System.Environment]::SetEnvironmentVariable("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1", "Process")

        Write-Host "âœ… Windows cleanup completed"

    - name: Run unit tests
      if: inputs.test-type == 'unit'
      id: run-tests
      shell: bash
      run: |
        echo "Running unit tests with enhanced error reporting..."
        echo "Python version: $(python --version)"
        echo "UV version: $(uv --version)"
        echo "Working directory: $(pwd)"
        echo "Available memory: $(free -h 2>/dev/null || echo 'N/A')"

        # Set test result output
        echo "result=success" >> $GITHUB_OUTPUT
        echo "coverage-file=coverage.xml" >> $GITHUB_OUTPUT

        # Verify dependencies are installed
        uv pip list | head -10

        # Run tests with detailed output and continue on failure for better debugging
        set +e  # Don't exit on failure to capture exit code

        # Platform-specific pytest configuration with enhanced stability
        if [[ "${{ runner.os }}" == "Windows" ]]; then
          # Windows-specific flags to reduce file contention and improve stability
          PYTEST_FLAGS="-n 1 --cache-clear --disable-warnings --tb=short --no-header --strict-config"
          export PYTEST_CURRENT_TEST="1"
          export PYTHONDONTWRITEBYTECODE="1"
        elif [[ "${{ runner.os }}" == "macOS" ]]; then
          # macOS-specific flags for stability (optimized for M1/Intel)
          PYTEST_FLAGS="-n 2 --cache-clear --tb=short --disable-warnings --no-header --strict-config"
        else
          # Linux systems can use optimized parallel execution
          PYTEST_FLAGS="-n auto --cache-clear --tb=short --no-header --strict-config"
        fi

        # Run optimized test suite with better performance and reliability
        timeout ${{ inputs.timeout }} uv run pytest tests/ \
          -m "not performance and not flaky and not ci_exclude and not notebook" \
          --ignore=tests/performance/ \
          --ignore=tests/notebooks/ \
          --cov=rxiv_maker \
          --cov-report=xml:coverage.xml \
          --cov-report=term-missing \
          --cov-report=html:htmlcov \
          $PYTEST_FLAGS \
          --maxfail=${{ inputs.max-failures }} \
          --tb=line \
          -v \
          --durations=10 \
          --strict-markers \
          --junit-xml=pytest-results.xml \
          --timeout=120 \
          --timeout-method=thread \
          || true  # Continue on test failures to capture results
        TEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error

        # Set result based on exit code
        if [ $TEST_EXIT_CODE -ne 0 ]; then
          echo "result=failure" >> $GITHUB_OUTPUT
        fi

        # Report test results even if some failed
        if [ -f "pytest-results.xml" ]; then
          echo "ðŸ“Š Test results summary:"
          grep -o 'tests="[0-9]*"' pytest-results.xml | head -1
          grep -o 'failures="[0-9]*"' pytest-results.xml | head -1
          grep -o 'errors="[0-9]*"' pytest-results.xml | head -1
        fi

        # Exit with original test exit code if tests failed
        exit ${TEST_EXIT_CODE:-0}
      env:
        PYTHONIOENCODING: utf-8
        PYTHONUTF8: 1
        PYTEST_TIMEOUT: ${{ inputs.timeout }}
        # Windows-specific environment for better file handling
        PYTEST_CURRENT_TEST: 1
        TMPDIR: ${{ runner.temp }}
        TMP: ${{ runner.temp }}
        TEMP: ${{ runner.temp }}

    - name: Run integration tests
      if: inputs.test-type == 'integration'
      shell: bash
      run: |
        echo "Running integration tests..."
        uv run pytest tests/integration/ -v --tb=short || echo "Integration tests not available"


    - name: Run security scans
      if: inputs.test-type == 'security'
      shell: bash
      run: |
        echo "ðŸ”’ Running security scans..."

        # Install security tools
        python -m pip install --upgrade pip bandit safety

        # Bandit scan
        if [ -d "src" ]; then
          bandit -r src/ -f json -o bandit-report.json || echo "Bandit completed with issues"
        fi

        # Safety check
        safety check --json --output safety-report.json || echo "Safety completed with issues"

        echo "âœ… Security scans completed"

    - name: Test CLI functionality
      if: inputs.test-type == 'unit'
      shell: bash
      run: |
        echo "Testing CLI installation and basic functionality..."
        uv run rxiv --version
        uv run rxiv --help
        uv run rxiv config show || echo "Config command not available"

    - name: Test package installation
      if: inputs.test-type == 'unit'
      shell: bash
      run: |
        # Build and test wheel installation
        uv build
        python -m pip install --upgrade pip
        python -m pip install dist/*.whl
        python -c "import rxiv_maker; print('âœ… Package imports successfully')"
        python -c "from rxiv_maker import __version__; print(f'Version: {__version__}')"

    - name: Upload coverage to Codecov
      if: inputs.upload-coverage == 'true' && inputs.python-version == '3.11' && inputs.os == 'ubuntu-latest'
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
